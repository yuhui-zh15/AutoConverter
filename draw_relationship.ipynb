{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(1234)\n",
    "def parse_multi_choice_response(response, all_choices, index2ans):\n",
    "    \"\"\"\n",
    "    Parse the prediction from the generated response.\n",
    "    Return the predicted index e.g., A, B, C, D.\n",
    "    \"\"\"\n",
    "    for char in [\",\", \".\", \"!\", \"?\", \";\", \":\", \"'\"]:\n",
    "        response = response.strip(char)\n",
    "    response = \" \" + response + \" \"  # add space to avoid partial match\n",
    "\n",
    "    index_ans = True\n",
    "    ans_with_brack = False\n",
    "    candidates = []\n",
    "    for choice in all_choices:  # e.g., (A) (B) (C) (D)\n",
    "        if f\"({choice})\" in response:\n",
    "            candidates.append(choice)\n",
    "            ans_with_brack = True\n",
    "\n",
    "    if len(candidates) == 0:\n",
    "        for choice in all_choices:  # e.g., A B C D\n",
    "            if f\" {choice} \" in response:\n",
    "                candidates.append(choice)\n",
    "\n",
    "    # if all above doesn't get candidates, check if the content is larger than 5 tokens and try to parse the example\n",
    "    if len(candidates) == 0 and len(response.split()) > 5:\n",
    "        for index, ans in index2ans.items():\n",
    "            if ans.lower() in response.lower():\n",
    "                candidates.append(index)\n",
    "                index_ans = False  # it's content ans.\n",
    "\n",
    "    if len(candidates) == 0:  # still not get answer, randomly choose one.\n",
    "        pred_index = random.choice(all_choices)\n",
    "    elif len(candidates) > 1:\n",
    "        start_indexes = []\n",
    "        if index_ans:\n",
    "            if ans_with_brack:\n",
    "                for can in candidates:\n",
    "                    index = response.rfind(f\"({can})\")\n",
    "                    start_indexes.append(index)  # -1 will be ignored anyway\n",
    "                # start_indexes = [generated_response.index(f'({can})') for can in candidates]\n",
    "            else:\n",
    "                for can in candidates:\n",
    "                    index = response.rfind(f\" {can} \")\n",
    "                    start_indexes.append(index)\n",
    "        else:\n",
    "            for can in candidates:\n",
    "                index = response.lower().rfind(index2ans[can].lower())\n",
    "                start_indexes.append(index)\n",
    "        # get the last one\n",
    "        pred_index = candidates[np.argmax(start_indexes)]\n",
    "    else:  # if only one candidate, use it.\n",
    "        pred_index = candidates[0]\n",
    "\n",
    "    return pred_index\n",
    "\n",
    "\n",
    "# parse_multi_choice_response(\n",
    "#     \"So the correct answer is bbbbb.\",\n",
    "#     [\"A\", \"B\", \"C\", \"D\"],\n",
    "#     {\"A\": \"aaaaa\", \"B\": \"bbbbb\", \"C\": \"ccccc\", \"D\": \"ddddd\"},\n",
    "# )\n",
    "\n",
    "def evaluate_accuracy(xlsx_file):\n",
    "    data = pd.read_excel(xlsx_file).to_dict(\"records\")\n",
    "    accs = []\n",
    "    for item in data:\n",
    "        # if \"choice_length\" in item:\n",
    "        #     all_choices = [chr(i) for i in range(65, 65 + item[\"choice_length\"])]\n",
    "        # else:\n",
    "        all_choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "        all_choices_mapping = {chr(i): str(item[chr(i)]) for i in range(65, 65 + len(all_choices))}\n",
    "        pred_index = parse_multi_choice_response(str(item[\"prediction\"]), all_choices, all_choices_mapping)\n",
    "\n",
    "        accs.append(pred_index == item[\"answer\"])\n",
    "\n",
    "    return sum(accs) / len(accs)\n",
    "\n",
    "# evaluate_accuracy(\"./yuchang_records_new/cambrian_8b/MathVista-357-MC_mc.xlsx\")\n",
    "# evaluate_accuracy(\"./yuchang_records_new/cambrian_8b/MMMU-1023-MC_mc.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llava_v1.5_7b': {'acc_original': 0.3605150214592275,\n",
       "  'acc_naive': 0.3954154727793696},\n",
       " 'Qwen2-VL-7B-Instruct': {'acc_original': 0.3090128755364807,\n",
       "  'acc_naive': 0.30945558739255014},\n",
       " 'Phi-3.5-Vision': {'acc_original': 0.4334763948497854,\n",
       "  'acc_naive': 0.5085959885386819},\n",
       " 'paligemma-3b-mix-448': {'acc_original': 0.3261802575107296,\n",
       "  'acc_naive': 0.31805157593123207}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# models = sorted([dir for dir in os.listdir(\"/pasteur2/u/yuhuiz/CVPR/AutoConverter/VLMEvalKit/outputs\") if \".csv\" not in dir])\n",
    "\n",
    "base_dir = \"/pasteur2/u/yuhuiz/CVPR/AutoConverter/VLMEvalKit/outputs\"\n",
    "models = [\"llava_v1.5_7b\", \"Qwen2-VL-7B-Instruct\", \"Phi-3.5-Vision\", \"paligemma-3b-mix-448\"]\n",
    "\n",
    "dataset = \"MMMU_DEV_VAL_4choices\"\n",
    "\n",
    "model2acc = {}\n",
    "for model in models:\n",
    "    model2acc[model] = {}\n",
    "    xlsx_file_original = f\"{base_dir}/{model}/{model}_{dataset}_original.xlsx\"\n",
    "    xlsx_file_naive = f\"{base_dir}/{model}/{model}_{dataset}_naive.xlsx\"\n",
    "    if os.path.exists(xlsx_file_original):\n",
    "        acc_original = evaluate_accuracy(xlsx_file_original)\n",
    "        model2acc[model][\"acc_original\"] = acc_original\n",
    "    if os.path.exists(xlsx_file_naive):\n",
    "        acc_naive = evaluate_accuracy(xlsx_file_naive)\n",
    "        model2acc[model][\"acc_naive\"] = acc_naive\n",
    "    \n",
    "\n",
    "display(model2acc)\n",
    "\n",
    "    # xlsx_file_agent = f\"./yuchang_records_new/{model}/{dataset}-1403_mc.xlsx\"\n",
    "    # xlsx_file_original = f\"./yuchang_records_new/{model}/{dataset}_mc.xlsx\"\n",
    "    # xlsx_file_baseline = f\"./yuchang_records_new/{model}/{dataset}-4-v0_mc.xlsx\"\n",
    "    # if os.path.exists(xlsx_file_agent) and os.path.exists(xlsx_file_original) and os.path.exists(xlsx_file_baseline):\n",
    "    #     acc_agent = evaluate_accuracy(xlsx_file_agent)\n",
    "    #     acc_original = evaluate_accuracy(xlsx_file_original)\n",
    "    #     acc_baseline = evaluate_accuracy(xlsx_file_baseline)\n",
    "    #     model2acc[model] = {\"acc_agent\": acc_agent, \"acc_original\": acc_original, \"acc_baseline\": acc_baseline}\n",
    "\n",
    "# # draw a scatter plot to show acc_original (y-axis) vs acc_agent (x-axis) for each model\n",
    "# from matplotlib.axes import Axes\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.rcParams[\"figure.dpi\"] = 300\n",
    "# plt.rcParams[\"savefig.dpi\"] = 300\n",
    "# sns.set_theme()\n",
    "# sns.set_context(\"paper\")  # paper, notebook, talk, and poster\n",
    "\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.scatter(\n",
    "#     [v[\"acc_agent\"] for v in model2acc.values()],\n",
    "#     [v[\"acc_original\"] for v in model2acc.values()],\n",
    "#     label=\"Agent vs Original\"\n",
    "# )\n",
    "\n",
    "# # draw the model name on the plot\n",
    "# for model, accs in model2acc.items():\n",
    "#     plt.text(accs[\"acc_agent\"], accs[\"acc_original\"], model, fontsize=2)\n",
    "\n",
    "# plt.scatter(\n",
    "#     [v[\"acc_baseline\"] for v in model2acc.values()],\n",
    "#     [v[\"acc_original\"] for v in model2acc.values()],\n",
    "#     label=\"Naive vs Original\"\n",
    "# )\n",
    "\n",
    "# min_val = min([v[\"acc_original\"] for v in model2acc.values()]) - 0.1\n",
    "# max_val = max([v[\"acc_original\"] for v in model2acc.values()]) + 0.1\n",
    "\n",
    "# # draw y = x from min to max\n",
    "# plt.plot([min_val, max_val], [min_val, max_val], color=\"black\", label=\"y=x\")\n",
    "\n",
    "# plt.xlabel(\"Accuracy of Converted Dataset\")\n",
    "# plt.ylabel(\"Accuracy of Original Dataset\")\n",
    "# plt.title(f\"Dataset: {dataset}\")\n",
    "# plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
