{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently supported datasets: ['COCO_VAL', 'MME', 'HallusionBench', 'POPE', 'AMBER', 'MMBench_DEV_EN', 'MMBench_TEST_EN', 'MMBench_DEV_CN', 'MMBench_TEST_CN', 'MMBench', 'MMBench_CN', 'MMBench_DEV_EN_V11', 'MMBench_TEST_EN_V11', 'MMBench_DEV_CN_V11', 'MMBench_TEST_CN_V11', 'MMBench_V11', 'MMBench_CN_V11', 'SEEDBench_IMG', 'SEEDBench2', 'SEEDBench2_Plus', 'ScienceQA_VAL', 'ScienceQA_TEST', 'MMT-Bench_ALL_MI', 'MMT-Bench_ALL', 'MMT-Bench_VAL_MI', 'MMT-Bench_VAL', 'AesBench_VAL', 'AesBench_TEST', 'Q-Bench1_VAL', 'Q-Bench1_TEST', 'A-Bench_VAL', 'A-Bench_TEST', 'R-Bench-Dis', 'R-Bench-Ref', 'CCBench', 'AI2D_TEST', 'AI2D_TEST_NO_MASK', 'MMStar', 'RealWorldQA', 'MLLMGuard_DS', 'BLINK', 'TaskMeAnything_v1_imageqa_random', 'A-OKVQA', 'MMMB_ar', 'MMMB_cn', 'MMMB_en', 'MMMB_pt', 'MMMB_ru', 'MMMB_tr', 'MMBench_dev_ar', 'MMBench_dev_cn', 'MMBench_dev_en', 'MMBench_dev_pt', 'MMBench_dev_tr', 'MMBench_dev_ru', 'OCRVQA_TEST', 'OCRVQA_TESTCORE', 'TextVQA_VAL', 'DocVQA_VAL', 'DocVQA_TEST', 'InfoVQA_VAL', 'InfoVQA_TEST', 'ChartQA_TEST', 'GQA_TestDev_Balanced', 'MathVision', 'MathVision_MINI', 'MMMU_DEV_VAL', 'MMMU_TEST', 'MMMU_DEV_VAL_4choices_naive', 'MMMU_DEV_VAL_4choices_original', 'MMMU_DEV_VAL_4choices_20241023_0056', 'MMMU_DEV_VAL_4choices_20241102_1700', 'OCRBench', 'MathVista_MINI', 'MathVista_MINI_4choices_original', 'LLaVABench', 'MMVet', 'MTVQA_TEST', 'TableVQABench', 'MMLongBench_DOC', 'VCR_EN_EASY_500', 'VCR_EN_EASY_100', 'VCR_EN_EASY_ALL', 'VCR_EN_HARD_500', 'VCR_EN_HARD_100', 'VCR_EN_HARD_ALL', 'VCR_ZH_EASY_500', 'VCR_ZH_EASY_100', 'VCR_ZH_EASY_ALL', 'VCR_ZH_HARD_500', 'VCR_ZH_HARD_100', 'VCR_ZH_HARD_ALL', 'MMDU', 'DUDE', 'DUDE_MINI', 'SLIDEVQA_MINI', 'SLIDEVQA', 'MUIRBench', 'GMAI-MMBench_VAL', 'GMAI-MMBench_TEST', 'MME-RealWorld', 'MME-RealWorld-CN', 'HRBench4K', 'HRBench8K', 'CRPE_EXIST', 'CRPE_RELATION', 'MathVerse_MINI', 'MathVerse_MINI_Vision_Only', 'MathVerse_MINI_Vision_Dominant', 'MathVerse_MINI_Vision_Intensive', 'MathVerse_MINI_Text_Lite', 'MathVerse_MINI_Text_Dominant', 'MMBench-Video', 'Video-MME', 'MVBench', 'MVBench_MP4', 'MMBench_DEV_EN', 'MMBench_TEST_EN', 'MMBench_DEV_CN', 'MMBench_TEST_CN', 'MMBench', 'MMBench_CN', 'MMBench_DEV_EN_V11', 'MMBench_TEST_EN_V11', 'MMBench_DEV_CN_V11', 'MMBench_TEST_CN_V11', 'MMBench_V11', 'MMBench_CN_V11', 'SEEDBench_IMG', 'SEEDBench2', 'SEEDBench2_Plus', 'ScienceQA_VAL', 'ScienceQA_TEST', 'MMT-Bench_ALL_MI', 'MMT-Bench_ALL', 'MMT-Bench_VAL_MI', 'MMT-Bench_VAL', 'AesBench_VAL', 'AesBench_TEST', 'Q-Bench1_VAL', 'Q-Bench1_TEST', 'A-Bench_VAL', 'A-Bench_TEST', 'R-Bench-Dis', 'R-Bench-Ref', 'CCBench', 'AI2D_TEST', 'AI2D_TEST_NO_MASK', 'MMStar', 'RealWorldQA', 'MLLMGuard_DS', 'BLINK', 'TaskMeAnything_v1_imageqa_random', 'A-OKVQA', 'MMMB_ar', 'MMMB_cn', 'MMMB_en', 'MMMB_pt', 'MMMB_ru', 'MMMB_tr', 'MMBench_dev_ar', 'MMBench_dev_cn', 'MMBench_dev_en', 'MMBench_dev_pt', 'MMBench_dev_tr', 'MMBench_dev_ru', 'MMMB', 'MTL_MMBench_DEV']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-02 20:26:09] ERROR - misc.py: load_env - 168: Did not detect the .env file at /pasteur2/u/yuhuiz/CVPR/AutoConverter/VLMEvalKit/.env, failed to load. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MMMU_DEV_VAL', 'MMMU_TEST', 'MMMU_DEV_VAL_4choices_naive', 'MMMU_DEV_VAL_4choices_original', 'MMMU_DEV_VAL_4choices_20241023_0056', 'MMMU_DEV_VAL_4choices_20241102_1700']\n",
      "dataset: MMMU_DEV_VAL, total: 1050, 4 choices: 665\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/pasteur2/u/yuhuiz/CVPR/AutoConverter/VLMEvalKit')\n",
    "\n",
    "from vlmeval import *\n",
    "\n",
    "######################### MMMU #########################\n",
    "dataset_name = 'MMMU_DEV_VAL'\n",
    "dataset = build_dataset(dataset_name)\n",
    "def get_n_choice(item):\n",
    "    choices = [item[idx] for idx in \"ABCDEFGHI\"]\n",
    "    choices = [choice for choice in choices if choice == choice]\n",
    "    return len(choices)\n",
    "dataset_4choices = [item for item in dataset if get_n_choice(item) == 4 and isinstance(item[\"image_path\"], str)]\n",
    "print(f\"dataset: {dataset_name}, total: {len(dataset)}, 4 choices: {len(dataset_4choices)}\")\n",
    "\n",
    "######################### MathVista #########################\n",
    "# dataset_name = 'MathVista_MINI'\n",
    "# dataset = build_dataset(dataset_name)\n",
    "# def get_n_choice(item):\n",
    "#     try:\n",
    "#         choices = eval(item[\"choices\"])\n",
    "#         return len(choices)\n",
    "#     except:\n",
    "#         return 0\n",
    "# dataset_4choices = [item for item in dataset if get_n_choice(item) == 4]\n",
    "# print(f\"dataset: {dataset_name}, total: {len(dataset)}, 4 choices: {len(dataset_4choices)}\")\n",
    "\n",
    "import json\n",
    "with open(f\"data/{dataset_name}_4choices_original.jsonl\", 'w') as f:\n",
    "    for item in dataset_4choices:\n",
    "        item[\"index\"] = int(item[\"index\"])\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from textwrap import dedent\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Distractor(BaseModel):\n",
    "    text: str\n",
    "    reason: str\n",
    "\n",
    "class Distractors(BaseModel):\n",
    "    distractors: list[Distractor]\n",
    "\n",
    "\n",
    "def base64_to_image(base64_str):\n",
    "    \"\"\"\n",
    "    Convert a base64 string to a PIL Image.\n",
    "    \n",
    "    Args:\n",
    "        base64_str (str): The base64 encoded image string.\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image.Image: The image object.\n",
    "    \"\"\"\n",
    "    # Decode the base64 string into bytes\n",
    "    image_data = base64.b64decode(base64_str)\n",
    "    \n",
    "    # Convert bytes into a PIL image\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "# def convert_to_multi_choice(item):\n",
    "#     question = item[\"question\"]\n",
    "#     answer = item[item[\"answer\"]]\n",
    "#     image_base64 = item[\"image\"]\n",
    "\n",
    "#     system_prompt = \"You are a helpful assistant.\"\n",
    "#     user_prompt = f\"\"\"Please generate 3 distractors for this question given the image:\n",
    "\n",
    "#     Question: {question}\n",
    "#     Answer: {answer}\n",
    "#     \"\"\"\n",
    "\n",
    "#     completion = client.beta.chat.completions.parse(\n",
    "#         model=\"gpt-4o\",\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": dedent(system_prompt)},\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\"type\": \"text\", \"text\": dedent(user_prompt)},\n",
    "#                     {\n",
    "#                         \"type\": \"image_url\",\n",
    "#                         \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"},\n",
    "#                     },\n",
    "#                 ],\n",
    "#             },\n",
    "#         ],\n",
    "#         response_format=Distractors,\n",
    "#     )\n",
    "\n",
    "#     distractors = completion.choices[0].message.parsed.dict()\n",
    "#     choices = [answer] + [distractor[\"text\"] for distractor in distractors[\"distractors\"]]\n",
    "#     reasons = [None] + [distractor[\"reason\"] for distractor in distractors[\"distractors\"]]\n",
    "#     multi_choice_questions = {\n",
    "#         \"question\": question,\n",
    "#         \"choices\": choices,\n",
    "#         \"reasons\": reasons,\n",
    "#         \"answer\": answer,\n",
    "#     }\n",
    "#     return multi_choice_questions\n",
    "\n",
    "\n",
    "def get_reply(system_prompt, user_prompt, image_base64, output_format):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": dedent(system_prompt)},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": dedent(user_prompt)},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"},\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        response_format=output_format,\n",
    "    )\n",
    "    parsed_output = completion.choices[0].message.parsed.dict()\n",
    "    return parsed_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from prompts import concept_generation_system_prompt, reasoning_generation_system_prompt, visual_interpretation_generation_system_prompt, data_processing_generation_system_prompt, question_bias_generation_system_prompt, fusion_generation_system_prompt, confuse_system_prompt\n",
    "def convert_to_multi_choice(item):\n",
    "    question = item[\"question\"]\n",
    "    answer = item[item[\"answer\"]]\n",
    "    image_base64 = item[\"image\"]\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Correct Answer: {answer}\n",
    "    \"\"\"\n",
    "\n",
    "    distractors_concept = get_reply(concept_generation_system_prompt, user_prompt, image_base64, Distractors)[\"distractors\"]\n",
    "    # print(distractors_concept)\n",
    "    distractors_reasoning = get_reply(reasoning_generation_system_prompt, user_prompt, image_base64, Distractors)[\"distractors\"]\n",
    "    distractors_visual_interpretation = get_reply(visual_interpretation_generation_system_prompt, user_prompt, image_base64, Distractors)[\"distractors\"]\n",
    "    distractors_data_processing = get_reply(data_processing_generation_system_prompt, user_prompt, image_base64, Distractors)[\"distractors\"]\n",
    "    distractors_question_bias = get_reply(question_bias_generation_system_prompt, user_prompt, image_base64, Distractors)[\"distractors\"]\n",
    "\n",
    "    # user_prompt = f\"\"\"\n",
    "    # Question: {question}\n",
    "    # Answer: {answer}\n",
    "\n",
    "    # Given the distractors generated already:\n",
    "    # ```{distractors_concept}```\n",
    "    \n",
    "    # Please think carefully about how to improve these distractors and refine these distractors.\n",
    "    # \"\"\"\n",
    "    # distractors_concept = get_reply(concept_generation_system_prompt, user_prompt, image_base64, Distractors)[\"distractors\"]\n",
    "    # print(distractors_concept)\n",
    "    \n",
    "\n",
    "    distractors = distractors_concept + distractors_reasoning + distractors_visual_interpretation + distractors_data_processing + distractors_question_bias\n",
    "    \n",
    "\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Correct Answer: {answer}\n",
    "    All Distractors: {distractors}\n",
    "    \"\"\"\n",
    "\n",
    "    distractors = get_reply(fusion_generation_system_prompt, user_prompt, image_base64, Distractors)[\"distractors\"]\n",
    "\n",
    "    # distractors = [distractor[\"text\"] for distractor in distractors]\n",
    "    # user_prompt = f\"\"\"\n",
    "    #     Question: {question}\n",
    "    #     Options: {distractors}\n",
    "    # \"\"\"\n",
    "\n",
    "    # distractors = get_reply(confuse_system_prompt, user_prompt, image_base64, Distractors)[\"distractors\"]\n",
    "\n",
    "    choices = [answer] + [distractor[\"text\"] for distractor in distractors]\n",
    "    reasons = [None] + [distractor[\"reason\"] for distractor in distractors]\n",
    "    multi_choice_questions = {\n",
    "        \"question\": question,\n",
    "        \"choices\": choices,\n",
    "        \"reasons\": reasons,\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "    return multi_choice_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Each of the following situations relates to a different company. <image 1> For company B, find the missing amounts.',\n",
       " 'choices': ['$77,490', '$87,490', '$69,350', '$127,490'],\n",
       " 'reasons': [None,\n",
       "  'A student might arrive at this number by incorrectly adding expenses instead of subtracting them from the revenue.',\n",
       "  'This option miscalculates by improperly distributing gains across different accounts, not adhering to initial figures.',\n",
       "  'By inaccurately overestimating the gain or underestimating the expenses, a student could mistakenly calculate this implausibly high net income.'],\n",
       " 'answer': '$77,490'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_multi_choice(dataset_4choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 665/665 [08:43<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# Deep copy the dataset\n",
    "dataset_4choices_naive = copy.deepcopy(dataset_4choices)\n",
    "\n",
    "# Function to process a single question item\n",
    "def process_item(qidx):\n",
    "    item = dataset_4choices_naive[qidx]\n",
    "    multi_choice_questions = convert_to_multi_choice(item)\n",
    "\n",
    "    choices = multi_choice_questions[\"choices\"]\n",
    "    answer = item[item[\"answer\"]]\n",
    "    random.shuffle(choices)\n",
    "    answer_idx = choices.index(answer)\n",
    "\n",
    "    for idx in range(len(choices)):\n",
    "        item[chr(65 + idx)] = choices[idx]\n",
    "    item[\"answer\"] = chr(65 + answer_idx)\n",
    "\n",
    "    return qidx, item\n",
    "\n",
    "# Parallelize using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Submit tasks for parallel execution, associating futures with their indices\n",
    "    futures = [executor.submit(process_item, qidx) for qidx in range(len(dataset_4choices_naive))]\n",
    "\n",
    "    # Create progress bar and track as futures complete\n",
    "    results = []\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        results.append(future.result())\n",
    "\n",
    "    # Sort results to maintain the original order\n",
    "    results = sorted(results, key=lambda x: x[0])\n",
    "\n",
    "# save results to data/MMMU_DEV_VAL_4choices_20241023_0056.jsonl\n",
    "import json\n",
    "output_filename = f\"data/{dataset_name}_4choices_20241102_2028.jsonl\"\n",
    "with open(output_filename, 'w') as f:\n",
    "    for _, item in results:\n",
    "        item[\"index\"] = int(item[\"index\"])\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# from collections import Counter\n",
    "\n",
    "# non_mc_dataset = [item for item in dataset if item[\"answer\"] not in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"]\n",
    "# mc_dataset = [item for item in dataset if item[\"answer\"] in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"]\n",
    "\n",
    "# def compute_n_choice(item):\n",
    "#     choices = [item[idx] for idx in \"ABCDEFGHI\"]\n",
    "#     # remove nan\n",
    "#     choices = [choice for choice in choices if choice == choice]\n",
    "#     return len(choices)\n",
    "\n",
    "\n",
    "# n_choices = [compute_n_choice(item) for item in dataset]\n",
    "# len(non_mc_dataset), len(mc_dataset)\n",
    "\n",
    "# plt.bar(Counter(n_choices).keys(), Counter(n_choices).values())\n",
    "# plt.title(\"Number of choices in MMMU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
