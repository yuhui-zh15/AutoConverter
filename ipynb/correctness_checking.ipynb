{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob \n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from textwrap import dedent\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "client = OpenAI(timeout=20)\n",
    "\n",
    "def base64_to_image(base64_str):\n",
    "    image_data = base64.b64decode(base64_str)\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    return image\n",
    "\n",
    "# annotation_dir = \"/pasteur2/u/suyc/VLMEval/AutoConverter/visualization\"\n",
    "# dataset_dir = \"/pasteur2/u/suyc/VLMEval/VLMEvalKit/LMUData/\"\n",
    "\n",
    "# annotations = []\n",
    "# for filename in glob.glob(annotation_dir + \"/*/*.xlsx\"):\n",
    "#     print(filename)\n",
    "#     basename = os.path.basename(filename)\n",
    "#     data = pd.read_csv(dataset_dir + basename.replace('_correctness.xlsx', '-50.tsv'), sep='\\t').to_dict('records')[:50]\n",
    "#     annotation = pd.read_excel(filename).to_dict('records')[:50]\n",
    "#     for item, anno in zip(data, annotation):\n",
    "#         anno[\"filename\"] = basename\n",
    "#         anno[\"image\"] = item[\"image\"]\n",
    "#     annotations += annotation\n",
    "\n",
    "# # json.dump(annotations, open('annotations_20241109_0042.json', 'w'), indent=4)\n",
    "# display(len(annotations), annotations[0])\n",
    "# display(base64_to_image(annotations[0][\"image\"]))\n",
    "\n",
    "annotations = json.load(open('annotations_20241109_0042.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Judgement(BaseModel):\n",
    "#     reasoning: str\n",
    "#     correctness: int\n",
    "\n",
    "\n",
    "# def judge_multichoice_correctness_with_image(image_base64: str, question: str, choices: list, correct_choice: str) -> str:\n",
    "#     system_prompt = f\"\"\"\n",
    "#     Your task is to evaluate a multiple-choice question (with accompanying image) to determine if any incorrect choices (distractors) could also be considered correct answers.\n",
    "\n",
    "#     CRITICAL: The marked correct answer MUST always be treated as valid and correct, regardless of your own assessment. Never question or evaluate the correct answer - your task is to accept it as an absolute truth and evaluate only whether other choices could also be correct.\n",
    "\n",
    "#     Score the question's correctness using this scale:\n",
    "#     5 - Perfect: All other choices are clearly incorrect\n",
    "#     4 - Good: Other choices are mostly wrong but have minor elements of correctness\n",
    "#     3 - Fair: At least one other choice could be partially correct\n",
    "#     2 - Poor: At least one other choice could be equally correct\n",
    "#     1 - Invalid: Multiple choices are equally valid as the correct answer\n",
    "\n",
    "#     Provide:\n",
    "#     1. Score (1-5)\n",
    "#     2. Brief explanation focusing specifically on any problematic distractor choices\n",
    "\n",
    "#     Remember: Never analyze whether the marked correct answer is right or wrong - it is ALWAYS correct by definition. Focus exclusively on whether other choices could also be valid answers.\n",
    "#     \"\"\"\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "#     Question: {question}\n",
    "#     Choices: {choices}\n",
    "#     Correct Answer: {correct_choice}\n",
    "#     \"\"\"\n",
    "\n",
    "#     response = client.beta.chat.completions.parse(\n",
    "#         model=\"gpt-4o\", \n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": dedent(system_prompt)}, # \"You are a helpful assistant.\"\n",
    "#             {\"role\": \"user\", \"content\": [\n",
    "#                 {\n",
    "#                     \"type\": \"text\",\n",
    "#                     \"text\": dedent(prompt)\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"image_url\",\n",
    "#                     \"image_url\": {\n",
    "#                         \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
    "#                     }\n",
    "#                 }\n",
    "#             ]}\n",
    "#         ],\n",
    "#         response_format=Judgement,\n",
    "#         temperature=0  # Set to 0 for deterministic responses\n",
    "#     )\n",
    "    \n",
    "#     answer = response.choices[0].message.parsed.dict()\n",
    "#     return answer\n",
    "\n",
    "\n",
    "\n",
    "class Judgement(BaseModel):\n",
    "    reasoning: str\n",
    "    correctness: int\n",
    "    improvement: str\n",
    "    \n",
    "def judge_multichoice_correctness_with_image(image_base64: str, question: str, choices: list, correct_choice: str) -> str:\n",
    "    system_prompt = f\"\"\"\n",
    "    Your task is to evaluate a multiple-choice question (with accompanying image) to determine if any incorrect choices (distractors) could also be considered correct answers.\n",
    "\n",
    "    CRITICAL: The marked correct answer MUST always be treated as valid and correct, regardless of your own assessment. Never question or evaluate the correct answer - your task is to accept it as an absolute truth and evaluate only whether other choices could also be correct.\n",
    "\n",
    "    Score the question's correctness using this scale:\n",
    "    5 - Perfect: All other choices are clearly incorrect\n",
    "    4 - Good: Other choices are mostly wrong but have minor elements of correctness\n",
    "    3 - Fair: At least one other choice could be partially correct\n",
    "    2 - Poor: At least one other choice could be equally correct\n",
    "    1 - Invalid: Multiple choices are equally valid as the correct answer\n",
    "\n",
    "    Provide:\n",
    "    1. Score (1-5)\n",
    "    2. Brief explanation focusing specifically on any problematic distractor choices\n",
    "    3. Suggested improvements for the problematic distractors (if applicable)\n",
    "\n",
    "    Remember: Never analyze whether the marked correct answer is right or wrong - it is ALWAYS correct by definition. Focus exclusively on whether other choices could also be valid answers.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Choices: {choices}\n",
    "    Correct Answer: {correct_choice}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": dedent(system_prompt)}, # \"You are a helpful assistant.\"\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": dedent(prompt)\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
    "                    }\n",
    "                }\n",
    "            ]}\n",
    "        ],\n",
    "        response_format=Judgement,\n",
    "        temperature=0  # Set to 0 for deterministic responses\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.parsed.dict()\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'The image shows a variety of vases and similar objects that appear to be made from clay, which aligns with pottery. However, ceramics is a broader term that includes pottery, so it could also be considered partially correct. Glassblowing and sculpture are clearly incorrect as the objects are not made of glass and do not represent typical sculptures.',\n",
       " 'correctness': 3,\n",
       " 'improvement': \"Replace 'ceramics' with a more distinct option like 'painting' to avoid overlap with 'pottery'.\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "item = annotations[0]\n",
    "prediction = judge_multichoice_correctness_with_image(item[\"image\"], item[\"question\"], [item[\"A\"], item[\"B\"], item[\"C\"], item[\"D\"]], item[item[\"answer\"]])\n",
    "display(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [01:01<00:00,  7.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# predictions = []\n",
    "# for item in tqdm(annotations):\n",
    "#     prediction = judge_multichoice_correctness_with_image(item[\"image\"], item[\"question\"], [item[\"A\"], item[\"B\"], item[\"C\"], item[\"D\"]], item[item[\"answer\"]])\n",
    "#     predictions.append(prediction)\n",
    "\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_item(item):\n",
    "    return judge_multichoice_correctness_with_image(\n",
    "        item[\"image\"],\n",
    "        item[\"question\"],\n",
    "        [item[\"A\"], item[\"B\"], item[\"C\"], item[\"D\"]],\n",
    "        item[item[\"answer\"]]\n",
    "    )\n",
    "\n",
    "def parallel_judge(annotations):\n",
    "    with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "        results = list(tqdm(executor.map(process_item, annotations), total=len(annotations)))\n",
    "    return results\n",
    "\n",
    "predictions = parallel_judge(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(predictions, open('predictions_20241109_2150_final.json', 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.7670507544581618\n",
      "Accuracy: 0.8777777777777778\n",
      "Precision 0.1875\n",
      "Recall 0.06666666666666667\n",
      "F1: 0.09836065573770492\n",
      "1 3 16\n",
      "2 12 23\n",
      "3 12 67\n",
      "4 7 26\n",
      "5 11 318\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "predictions = json.load(open('predictions_20241109_2150_final.json'))\n",
    "annotations = json.load(open('annotations_20241109_0042.json'))\n",
    "\n",
    "preds = [item[\"correctness\"] for item in predictions]\n",
    "labels = [item[\"correctness\"] for item in annotations]\n",
    "\n",
    "# compute accuracy, f1, auroc\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "auroc = roc_auc_score(labels, preds)\n",
    "preds_binary = [1 if pred >= 2 else 0 for pred in preds]\n",
    "\n",
    "accuracy = accuracy_score(labels, preds_binary)\n",
    "p, r, f1, _ = precision_recall_fscore_support(labels, preds_binary, average=None)\n",
    "precision = p[0]\n",
    "recall = r[0]\n",
    "f1 = f1[0]\n",
    "\n",
    "\n",
    "print(\"AUROC:\", auroc)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision\", precision)\n",
    "print(\"Recall\", recall)\n",
    "print(\"F1:\", f1)\n",
    "\n",
    "# if preds==1, print the percentage of questions are indeed wrong\n",
    "for i in range(1, 6):\n",
    "    selected_preds = [label for label, pred in zip(labels, preds) if pred==i]\n",
    "    print(i, selected_preds.count(0), len(selected_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (pred, anno) in enumerate(zip(predictions, annotations)):\n",
    "    if pred[\"correctness\"] == 1:  # and anno[\"correctness\"] == 0\n",
    "        display(idx)\n",
    "        display(anno[\"question\"], [anno[\"A\"], anno[\"B\"], anno[\"C\"], anno[\"D\"]], anno[anno[\"answer\"]]) # , pred[\"reasoning\"])\n",
    "        image = base64_to_image(anno[\"image\"])\n",
    "        display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pred, anno in zip(predictions, annotations):\n",
    "#     display(pred)\n",
    "#     display(anno)\n",
    "#     image = base64_to_image(anno[\"image\"]).resize((384, 384))\n",
    "#     display(image)\n",
    "#     input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/pasteur2/u/suyc/VLMEval/VLMEvalKit/LMUData/VMCBench-1000-v2.tsv\", sep='\\t').to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[336]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine Captialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    distractors: list[str]\n",
    "\n",
    "client = OpenAI(timeout=20)\n",
    "\n",
    "def refine_question_with_image(image_base64: str, question: str, choices: list, correct_choice: str) -> str:\n",
    "    system_prompt = f\"\"\"\n",
    "    Your task is to refine the distractors (incorrect options) of a multiple-choice question by matching their capitalization style with the correct answer:\n",
    "\n",
    "    1. Analyze the capitalization pattern of the correct answer (e.g., \"Full Capitals\", \"first Letter Only\", \"all lowercase\", \"camelCase\", \"Title Case\")\n",
    "    2. Modify each distractor to follow the same capitalization pattern as the correct answer\n",
    "    - Every word position should match (first word, middle words, last word)\n",
    "    - Special terms, acronyms, and proper nouns should maintain their standard capitalization\n",
    "    3. Do not make any other changes to the distractors' content, length, or meaning\n",
    "    4. Do not modify the correct answer or question itself\n",
    "\n",
    "    If the distractors already match the correct answer's capitalization pattern, leave them unchanged.\n",
    "\n",
    "    Please output only the refined distractors.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Correct Answer: ```{correct_choice}```\n",
    "    Distractors: ```{list(set(choices) - set([correct_choice]))}```\n",
    "    \"\"\"\n",
    "    print(prompt)\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": dedent(system_prompt)}, # \"You are a helpful assistant.\"\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": dedent(prompt)\n",
    "                },\n",
    "                # {\n",
    "                #     \"type\": \"image_url\",\n",
    "                #     \"image_url\": {\n",
    "                #         \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
    "                #     }\n",
    "                # }\n",
    "            ]}\n",
    "        ],\n",
    "        response_format=Question,\n",
    "        temperature=0  # Set to 0 for deterministic responses\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.parsed.dict()\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = annotations[103] # 3, 103, 200\n",
    "prediction = refine_question_with_image(item[\"image\"], item[\"question\"], [item[\"A\"], item[\"B\"], item[\"C\"], item[\"D\"]], item[item[\"answer\"]])\n",
    "display(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judgement(BaseModel):\n",
    "    reasoning: str\n",
    "    correctness: int\n",
    "    improvement: str\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    reasoning: str\n",
    "    distractors: list[str]\n",
    "\n",
    "\n",
    "client = OpenAI(timeout=20)\n",
    "\n",
    "def judge_multichoice_correctness_with_image(image_base64: str, question: str, choices: list, correct_choice: str) -> str:\n",
    "    system_prompt = f\"\"\"\n",
    "    Your task is to evaluate a multiple-choice question (with accompanying image) to determine if any incorrect choices (distractors) could also be considered correct answers.\n",
    "\n",
    "    CRITICAL: The marked correct answer MUST always be treated as valid and correct, regardless of your own assessment. Never question or evaluate the correct answer - your task is to accept it as an absolute truth and evaluate only whether other choices could also be correct.\n",
    "\n",
    "    Score the question's correctness using this scale:\n",
    "    5 - Perfect: All other choices are clearly incorrect\n",
    "    4 - Good: Other choices are mostly wrong but have minor elements of correctness\n",
    "    3 - Fair: At least one other choice could be partially correct\n",
    "    2 - Poor: At least one other choice could be equally correct\n",
    "    1 - Invalid: Multiple choices are equally valid as the correct answer\n",
    "\n",
    "    Provide:\n",
    "    1. Score (1-5)\n",
    "    2. Brief explanation focusing specifically on any problematic distractor choices\n",
    "    3. Suggested improvements for the problematic distractors (if applicable)\n",
    "\n",
    "    Remember: Never analyze whether the marked correct answer is right or wrong - it is ALWAYS correct by definition. Focus exclusively on whether other choices could also be valid answers.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Choices: {choices}\n",
    "    Correct Answer: {correct_choice}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": dedent(system_prompt)}, # \"You are a helpful assistant.\"\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": dedent(prompt)\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
    "                    }\n",
    "                }\n",
    "            ]}\n",
    "        ],\n",
    "        response_format=Judgement,\n",
    "        temperature=0  # Set to 0 for deterministic responses\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.parsed.dict()\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def improve_multichoice_correctness_with_image(image_base64: str, question: str, choices: list, correct_choice: str, issue: str, improvement: str) -> str:\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert in educational assessment design specializing in multiple-choice question improvement. Your task is to enhance question effectiveness by revising problematic distractors (incorrect answer choices) while maintaining the existing correct answer.\n",
    "\n",
    "    Input Required:\n",
    "    1. The complete question\n",
    "    2. The current correct answer\n",
    "    3. Any associated images/materials\n",
    "    4. Specific feedback about problematic distractors\n",
    "    5. Suggested improvements (if provided)\n",
    "\n",
    "    Analysis Steps:\n",
    "    1. Review the question content and learning objective\n",
    "    2. Analyze the designated correct answer\n",
    "    3. Examine the feedback regarding problematic distractors\n",
    "    4. Evaluate any provided suggestions for improvement:\n",
    "    - Assess if suggestions fully address the identified issues\n",
    "    - Determine if suggestions align with best practices\n",
    "    - Identify any gaps or weaknesses in the suggestions\n",
    "    5. Develop exactly 3 improved distractors that:\n",
    "    - Are plausible but clearly incorrect\n",
    "    - Address the identified issues\n",
    "    - Align with common student misconceptions\n",
    "    - Maintain consistent format and length with other options\n",
    "    - Go beyond provided suggestions when necessary for better quality\n",
    "\n",
    "    Guidelines:\n",
    "    1. Treat the marked correct answer as fixed and unchangeable\n",
    "    2. Only modify distractors specifically identified as problematic\n",
    "    3. Preserve any well-functioning distractors\n",
    "    4. Maintain the original difficulty level of the question\n",
    "    5. Use your expertise to improve upon or deviate from provided suggestions if they:\n",
    "    - Are too vague or incomplete\n",
    "    - Don't fully address the identified issues\n",
    "    - Could be enhanced for better assessment quality\n",
    "    - Miss important misconceptions or learning opportunities\n",
    "\n",
    "    Output:\n",
    "    1. Brief analysis of the distractor issues and improvement approach\n",
    "    2. Three improved distractors\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Choices: {choices}\n",
    "    Correct Answer: {correct_choice}\n",
    "    Identified Issues: {issue}\n",
    "    Suggested Improvements: {improvement}\n",
    "    \"\"\"\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": dedent(system_prompt)}, # \"You are a helpful assistant.\"\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": dedent(prompt)\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
    "                    }\n",
    "                }\n",
    "            ]}\n",
    "        ],\n",
    "        response_format=Question,\n",
    "        temperature=0  # Set to 0 for deterministic responses\n",
    "    )\n",
    "    \n",
    "    distractors = response.choices[0].message.parsed.dict()\n",
    "    return distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "item = deepcopy(annotations[373])\n",
    "display(item)\n",
    "\n",
    "################ Round 1 ################\n",
    "judgement = judge_multichoice_correctness_with_image(item[\"image\"], item[\"question\"], [item[\"A\"], item[\"B\"], item[\"C\"], item[\"D\"]], item[item[\"answer\"]])\n",
    "display(judgement)\n",
    "distractors = improve_multichoice_correctness_with_image(item[\"image\"], item[\"question\"], [item[\"A\"], item[\"B\"], item[\"C\"], item[\"D\"]], item[item[\"answer\"]], judgement[\"reasoning\"], judgement[\"improvement\"])\n",
    "display(distractors)\n",
    "answer = item[item[\"answer\"]]\n",
    "options = distractors[\"distractors\"] + [answer]\n",
    "random.shuffle(options)\n",
    "item[\"A\"], item[\"B\"], item[\"C\"], item[\"D\"] = options\n",
    "item[\"answer\"] = \"ABCD\"[options.index(answer)]\n",
    "display(\"-\" * 63)\n",
    "\n",
    "################ Round 2 ################\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
